{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machathon2-qualification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AjzV5kF_iAge",
        "4sJ8XXc_tfYS",
        "Ft5NOaY2LVem",
        "HmE15ONWvSLN",
        "btvqG92nts3M",
        "Ydi8ofgtuLWh",
        "EfNtQKyrDDCi"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjzV5kF_iAge"
      },
      "source": [
        "# IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zJVsNB4XRHu",
        "outputId": "1da8bebf-38c9-4191-f63b-b401af95e27c"
      },
      "source": [
        "!pip install pyarabic"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyarabic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
            "\r\u001b[K     |███                             | 10kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 20kB 25.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 30kB 29.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 40kB 30.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 51kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 61kB 33.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 71kB 28.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 81kB 30.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 92kB 31.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 102kB 32.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 32.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
            "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp37-none-any.whl size=113324 sha256=41e3efc989b5f189ea323d4da574cdd82ef6b7477ae52659f5dde90a73acb92c\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
            "Successfully built pyarabic\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfNTWdKRXW87",
        "outputId": "9c298c9a-4613-4c8f-9229-5817c2ad1453"
      },
      "source": [
        "!wget 'https://raw.githubusercontent.com/zaidalyafeai/ARBML/master/datasets/Poem Meters/baits.zip'\n",
        "!unzip baits.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-31 05:50:53--  https://raw.githubusercontent.com/zaidalyafeai/ARBML/master/datasets/Poem%20Meters/baits.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2267882 (2.2M) [application/zip]\n",
            "Saving to: ‘baits.zip’\n",
            "\n",
            "baits.zip           100%[===================>]   2.16M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-05-31 05:50:53 (44.6 MB/s) - ‘baits.zip’ saved [2267882/2267882]\n",
            "\n",
            "Archive:  baits.zip\n",
            "   creating: final_baits/\n",
            "  inflating: final_baits/train.txt   \n",
            "  inflating: final_baits/labels.txt  \n",
            "  inflating: final_baits/test.txt    \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmntvEcR0JLg",
        "outputId": "fe69158a-2437-4249-fbd6-646d49acc1ac"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKxb6HlitLgu"
      },
      "source": [
        "# Assesting libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from pyarabic import araby\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import GRU, Embedding, Dense, Input, Dropout, Bidirectional, BatchNormalization, Flatten, Reshape,LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18ZnPHX8tYhj"
      },
      "source": [
        "#Constants\n",
        "TOKENIZER_WORD_NUM = 50000\n",
        "#Files\n",
        "DATA_TRAIN_PATH = '/content/drive/MyDrive/dataset/train.csv'\n",
        "DATA_LABELS_PATH = '/content/drive/MyDrive/dataset/labels.txt'\n",
        "#Values\n",
        "MAX_VOCAB_SIZE = 50000\n",
        "EMBEDDING_DIM = 300\n",
        "DELTA = 1e-12\n",
        "BETA = 1e-6\n",
        "GAMMA = 1e-3"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sJ8XXc_tfYS"
      },
      "source": [
        "# Dataset Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGIzCSJ2td0i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "51b62e36-c8d1-4809-a7a2-abfa446f82e3"
      },
      "source": [
        "data = pd.read_csv(DATA_TRAIN_PATH)\n",
        "data.head()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>أَنا الفقير وباللَه العظيم غني # لئن فقدتك في ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>وَلوعاً بِيُمنَى نَمْنَمَتْها حَدِيقَةٌ # نَزْ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>فيا منْ لم أزلْ أحظى لديه # بفضلٍ جامعٍ بابَ ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>وَسَلامٌ عَلَى ضَرِيحِكَ مَا أَهْ # دَتْ شَذَا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>أمِنْتُ فقري لما قُلتُ عن ثِقَةٍ # أنْ لا جواد...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   labels                                               data\n",
              "0       8  أَنا الفقير وباللَه العظيم غني # لئن فقدتك في ...\n",
              "1      10  وَلوعاً بِيُمنَى نَمْنَمَتْها حَدِيقَةٌ # نَزْ...\n",
              "2      11  فيا منْ لم أزلْ أحظى لديه # بفضلٍ جامعٍ بابَ ا...\n",
              "3       9  وَسَلامٌ عَلَى ضَرِيحِكَ مَا أَهْ # دَتْ شَذَا...\n",
              "4       8  أمِنْتُ فقري لما قُلتُ عن ثِقَةٍ # أنْ لا جواد..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc1mArYUL2pX"
      },
      "source": [
        "#some raw open source dataset with 1.5 million verses on github that i cleaned in an external notebook file\n",
        "extra = pd.read_csv('/content/drive/MyDrive/dataset/extra_train.csv') \n",
        "\n",
        "data = pd.concat([data,extra],ignore_index=True)\n",
        "data = data.drop_duplicates()\n",
        "data = data.reset_index().drop(labels=[\"index\"], axis=1)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "T93dSpXjMuwo",
        "outputId": "4ffb8621-5f93-4eba-a084-59ce68703946"
      },
      "source": [
        "data"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>أَنا الفقير وباللَه العظيم غني # لئن فقدتك في ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>وَلوعاً بِيُمنَى نَمْنَمَتْها حَدِيقَةٌ # نَزْ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>فيا منْ لم أزلْ أحظى لديه # بفضلٍ جامعٍ بابَ ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>وَسَلامٌ عَلَى ضَرِيحِكَ مَا أَهْ # دَتْ شَذَا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>أمِنْتُ فقري لما قُلتُ عن ثِقَةٍ # أنْ لا جواد...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469362</th>\n",
              "      <td>9</td>\n",
              "      <td>هي أغلى ما أنشأ اللَّه في الدنيا # وأحلى قصيدة...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469363</th>\n",
              "      <td>9</td>\n",
              "      <td>هي أغرودة الأغاريد تنساب # كحلم يغشى الجفون ال...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469364</th>\n",
              "      <td>9</td>\n",
              "      <td>هي شلال بهجة وبهاء # يتداعى وجداً ويخفق حسنا</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469365</th>\n",
              "      <td>9</td>\n",
              "      <td>هي حلم الهوى ومنطلقي الباقي # يدك الحدود سجناً...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469366</th>\n",
              "      <td>9</td>\n",
              "      <td>هي حبي العاتي وكل غرامي # آه لو أدرك الغرام لجنا</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1469367 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         labels                                               data\n",
              "0             8  أَنا الفقير وباللَه العظيم غني # لئن فقدتك في ...\n",
              "1            10  وَلوعاً بِيُمنَى نَمْنَمَتْها حَدِيقَةٌ # نَزْ...\n",
              "2            11  فيا منْ لم أزلْ أحظى لديه # بفضلٍ جامعٍ بابَ ا...\n",
              "3             9  وَسَلامٌ عَلَى ضَرِيحِكَ مَا أَهْ # دَتْ شَذَا...\n",
              "4             8  أمِنْتُ فقري لما قُلتُ عن ثِقَةٍ # أنْ لا جواد...\n",
              "...         ...                                                ...\n",
              "1469362       9  هي أغلى ما أنشأ اللَّه في الدنيا # وأحلى قصيدة...\n",
              "1469363       9  هي أغرودة الأغاريد تنساب # كحلم يغشى الجفون ال...\n",
              "1469364       9       هي شلال بهجة وبهاء # يتداعى وجداً ويخفق حسنا\n",
              "1469365       9  هي حلم الهوى ومنطلقي الباقي # يدك الحدود سجناً...\n",
              "1469366       9   هي حبي العاتي وكل غرامي # آه لو أدرك الغرام لجنا\n",
              "\n",
              "[1469367 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2TmE6a3Aq8K"
      },
      "source": [
        "labels =  pd.read_csv(DATA_LABELS_PATH,\n",
        "                   sep=\"\\n\",\n",
        "                   header=None,\n",
        "                   engine='python').values"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHkAEshvByLr"
      },
      "source": [
        "def get_label(index,labels):\n",
        "  return labels[index][0]"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vNpxePH_eqV",
        "outputId": "8d59b0b3-c8a7-4cd4-f731-dcfdccfc0633"
      },
      "source": [
        "for i in data.iloc[:10]['data']:\n",
        "  print(i,end='\\n---------------------------------------------------------\\n')"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "أَنا الفقير وباللَه العظيم غني # لئن فقدتك في أَشياء طلتُ بها\n",
            "---------------------------------------------------------\n",
            "وَلوعاً بِيُمنَى نَمْنَمَتْها حَدِيقَةٌ # نَزْهَدُ أَحْدَاق الوَرَى فِي الحَدَائِقِ\n",
            "---------------------------------------------------------\n",
            "فيا منْ لم أزلْ أحظى لديه # بفضلٍ جامعٍ بابَ الزِّياده\n",
            "---------------------------------------------------------\n",
            "وَسَلامٌ عَلَى ضَرِيحِكَ مَا أَهْ # دَتْ شَذَاهَا حَدِيقَةٌ غَلْبَاءُ\n",
            "---------------------------------------------------------\n",
            "أمِنْتُ فقري لما قُلتُ عن ثِقَةٍ # أنْ لا جواد سوى السلطان مسعودِ\n",
            "---------------------------------------------------------\n",
            "كأنه بضمير الركضِ يضْربهُ # يدنو عليه بعيد الأرض مُرتكضاً\n",
            "---------------------------------------------------------\n",
            "عَزَّ مَن أَمدحُهُ في رَجبٍ # فَأَنا الأَخْرَسُ والشَّهْرُ الأَصَمْ\n",
            "---------------------------------------------------------\n",
            "إِذا ما عاجِزٌ رَثَّت قُواهُ # رَأى وَطءَ الفِراشِ لَهُ فَناما\n",
            "---------------------------------------------------------\n",
            "فإنما أنا ليثٌ # عادٍ وأنت كُليبُ\n",
            "---------------------------------------------------------\n",
            "لم نرضَ ما وفى من الأماني # حتَّى شفعناه بصيدٍ ثاني\n",
            "---------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ryEzKky_xT7"
      },
      "source": [
        "# apply label\n",
        "data['labels_text']=data['labels'].apply(lambda x: get_label(x,labels))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "TPcnw4VRCtxM",
        "outputId": "a4af72a8-b552-4c91-a980-300d7abad998"
      },
      "source": [
        "data"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>data</th>\n",
              "      <th>labels_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>أَنا الفقير وباللَه العظيم غني # لئن فقدتك في ...</td>\n",
              "      <td>baseet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>وَلوعاً بِيُمنَى نَمْنَمَتْها حَدِيقَةٌ # نَزْ...</td>\n",
              "      <td>taweel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>فيا منْ لم أزلْ أحظى لديه # بفضلٍ جامعٍ بابَ ا...</td>\n",
              "      <td>wafer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>وَسَلامٌ عَلَى ضَرِيحِكَ مَا أَهْ # دَتْ شَذَا...</td>\n",
              "      <td>khafeef</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>أمِنْتُ فقري لما قُلتُ عن ثِقَةٍ # أنْ لا جواد...</td>\n",
              "      <td>baseet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469362</th>\n",
              "      <td>9</td>\n",
              "      <td>هي أغلى ما أنشأ اللَّه في الدنيا # وأحلى قصيدة...</td>\n",
              "      <td>khafeef</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469363</th>\n",
              "      <td>9</td>\n",
              "      <td>هي أغرودة الأغاريد تنساب # كحلم يغشى الجفون ال...</td>\n",
              "      <td>khafeef</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469364</th>\n",
              "      <td>9</td>\n",
              "      <td>هي شلال بهجة وبهاء # يتداعى وجداً ويخفق حسنا</td>\n",
              "      <td>khafeef</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469365</th>\n",
              "      <td>9</td>\n",
              "      <td>هي حلم الهوى ومنطلقي الباقي # يدك الحدود سجناً...</td>\n",
              "      <td>khafeef</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469366</th>\n",
              "      <td>9</td>\n",
              "      <td>هي حبي العاتي وكل غرامي # آه لو أدرك الغرام لجنا</td>\n",
              "      <td>khafeef</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1469367 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         labels                                               data labels_text\n",
              "0             8  أَنا الفقير وباللَه العظيم غني # لئن فقدتك في ...      baseet\n",
              "1            10  وَلوعاً بِيُمنَى نَمْنَمَتْها حَدِيقَةٌ # نَزْ...      taweel\n",
              "2            11  فيا منْ لم أزلْ أحظى لديه # بفضلٍ جامعٍ بابَ ا...       wafer\n",
              "3             9  وَسَلامٌ عَلَى ضَرِيحِكَ مَا أَهْ # دَتْ شَذَا...     khafeef\n",
              "4             8  أمِنْتُ فقري لما قُلتُ عن ثِقَةٍ # أنْ لا جواد...      baseet\n",
              "...         ...                                                ...         ...\n",
              "1469362       9  هي أغلى ما أنشأ اللَّه في الدنيا # وأحلى قصيدة...     khafeef\n",
              "1469363       9  هي أغرودة الأغاريد تنساب # كحلم يغشى الجفون ال...     khafeef\n",
              "1469364       9       هي شلال بهجة وبهاء # يتداعى وجداً ويخفق حسنا     khafeef\n",
              "1469365       9  هي حلم الهوى ومنطلقي الباقي # يدك الحدود سجناً...     khafeef\n",
              "1469366       9   هي حبي العاتي وكل غرامي # آه لو أدرك الغرام لجنا     khafeef\n",
              "\n",
              "[1469367 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft5NOaY2LVem"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwNp-FpBkqpE"
      },
      "source": [
        "char2index = {' ': 1,'#': 2,'ء': 3,'آ': 4,'أ': 5,'ؤ': 6,'إ': 7,'ئ': 8,'ا': 9,'ب': 10,'ة': 11,'ت': 12,'ث': 13,'ج': 14,'ح': 15,'خ': 16,'د': 17,'ذ': 18,'ر': 19,'ز': 20,'س': 21,'ش': 22,'ص': 23,'ض': 24,'ط': 25,'ظ': 26,'ع': 27,'غ': 28,'ف': 29,'ق': 30,'ك': 31,'ل': 32,'م': 33,'ن': 34,'ه': 35,'و': 36,'ى': 37,'ي': 38}"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yON1PfrKLYk6"
      },
      "source": [
        "def cleanSentence(string):\n",
        "\n",
        "  string = araby.strip_tashkeel(string)\n",
        "  string = re.sub(r'[…1423567890\"–_!()*-.ـ:=o«»;\\[\\]؛,،~?؟\\u200f\\ufeffـ\\u200d\\u200c\\uf020\\uf03a\\uf02d\\uf02e]*','',string)\n",
        "  string = re.sub(r'[abcdefghijklmnopqrstuvwx×yzABCDEFGHIJKLMNOPQRSTUVWXYZ]*','',string)\n",
        "  string = re.sub(r\"\\s+$\", '',string)\n",
        "  string = re.sub(r\"^\\s+\", '',string)\n",
        "\n",
        "  return string"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKvLr1AGlBaX"
      },
      "source": [
        "def tokenize(string):\n",
        "  return [char2index[char] for char in string]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ws03gWfVpNp"
      },
      "source": [
        "samples_data = data.sample(n=1000000, random_state=41)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqUMK3uOTpSh",
        "outputId": "95e5b419-81d1-4382-ece0-9caba9ec3d57"
      },
      "source": [
        "tokenized_matrix = pad_sequences(samples_data['data'].apply(lambda x: tokenize(cleanSentence(x))).values,padding='post', value=0, maxlen = 100)\n",
        "tokenized_matrix"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[15, 21, 10, ...,  0,  0,  0],\n",
              "       [36, 12, 23, ...,  0,  0,  0],\n",
              "       [36, 21, 38, ...,  0,  0,  0],\n",
              "       ...,\n",
              "       [33, 15, 33, ...,  0,  0,  0],\n",
              "       [ 5, 24, 15, ...,  0,  0,  0],\n",
              "       [34, 35, 36, ...,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKlHLOA2jLx2"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwMmQHn1iFGj"
      },
      "source": [
        "X_train, X_valid , y_train, y_valid = train_test_split(tokenized_matrix, samples_data['labels'].values, test_size = 0.10, random_state = 41)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RUQXsdXRNSN",
        "outputId": "a30cfa0c-c4c9-4aea-c913-8744c4b5423b"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "900000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNqS5O3QoqFL",
        "outputId": "4e756863-894b-4c95-fbe1-7516f239920c"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input((100,)))\n",
        "model.add(Embedding(len(char2index)+1, 512))\n",
        "model.add(Bidirectional(GRU(units = 256, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(units = 256, return_sequences=True)))\n",
        "model.add(Bidirectional(GRU(units = 256)))\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(len(labels), activation = 'softmax'))\n",
        "\n",
        "# batch_size = 64 # Batch size for training.\n",
        "# epochs = 100  # Number of epochs to train for.\n",
        "# latent_dim = 64  # Latent dimensionality of the encoding space.\n",
        "\n",
        "# model = Sequential()\n",
        "# model.add(Embedding(len(char2index)+1, 32, input_length=100, mask_zero=True))\n",
        "# model.add(Bidirectional(LSTM(latent_dim, input_shape=(None,len(char2index)), return_sequences=True,\n",
        "#             dropout=0.1, recurrent_dropout=0.3),\n",
        "#             merge_mode='concat'))\n",
        "# model.add(Bidirectional(LSTM(latent_dim, return_sequences=True,\n",
        "#             dropout=0.1, recurrent_dropout=0.3),\n",
        "#             merge_mode='concat'))\n",
        "# model.add(Bidirectional(LSTM(latent_dim, return_sequences=True,\n",
        "#             dropout=0.1, recurrent_dropout=0.3),\n",
        "#             merge_mode='concat'))\n",
        "# model.add(Bidirectional(LSTM(latent_dim,\n",
        "#             dropout=0.1, recurrent_dropout=0.3),\n",
        "#             merge_mode='concat'))\n",
        "# model.add(Dense(len(labels), activation='softmax'))\n",
        "model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq6uHI4ApBqf",
        "outputId": "a1d901a8-28b5-4b55-a63e-5559ea056ff3"
      },
      "source": [
        "model(tf.zeros((10, 100))).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([10, 14])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_shaUZCotHa",
        "outputId": "f2900db4-0ef4-42ff-8800-655c83c9b17f"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 512)          19968     \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 100, 512)          1182720   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 100, 512)          1182720   \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 512)               1182720   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 14)                1806      \n",
            "=================================================================\n",
            "Total params: 3,635,598\n",
            "Trainable params: 3,635,598\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE0HZeZFpF1A"
      },
      "source": [
        "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_delta=0.0001, min_lr=0.0001)]\n",
        "callbacks += [tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/dataset/final_model.h5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')]\n",
        "callbacks += [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=5)]\n",
        "\n",
        "# checkpoint_path = \"cp.ckpt\"\n",
        "\n",
        "# callbacks_list = [\n",
        "#     tf.keras.callbacks.EarlyStopping(\n",
        "#         monitor='val_accuracy',\n",
        "#         patience=5),\n",
        "#     tf.keras.callbacks.ModelCheckpoint(\n",
        "#         'full_verse.h5',\n",
        "#         mode='max',\n",
        "#         save_best_only=True,\n",
        "#         monitor='val_accuracy'),\n",
        "# ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUBCJj8wpVJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f39c70b0-eaac-4f4a-c751-a0f6054a55e5"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data= (X_valid, y_valid), epochs = 50, batch_size= 128, shuffle = True, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "7032/7032 [==============================] - 950s 134ms/step - loss: 0.3806 - accuracy: 0.8893 - val_loss: 0.1989 - val_accuracy: 0.9511\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.95109, saving model to /content/drive/MyDrive/dataset/final_model.h5\n",
            "Epoch 2/50\n",
            "7032/7032 [==============================] - 943s 134ms/step - loss: 0.2002 - accuracy: 0.9531 - val_loss: 0.1858 - val_accuracy: 0.9549\n",
            "\n",
            "Epoch 00002: val_accuracy improved from 0.95109 to 0.95486, saving model to /content/drive/MyDrive/dataset/final_model.h5\n",
            "Epoch 3/50\n",
            "7032/7032 [==============================] - 942s 134ms/step - loss: 0.2033 - accuracy: 0.9518 - val_loss: 0.2015 - val_accuracy: 0.9507\n",
            "\n",
            "Epoch 00003: val_accuracy did not improve from 0.95486\n",
            "Epoch 4/50\n",
            "7032/7032 [==============================] - 943s 134ms/step - loss: 0.2232 - accuracy: 0.9449 - val_loss: 0.2291 - val_accuracy: 0.9404\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.95486\n",
            "Epoch 5/50\n",
            "2632/7032 [==========>...................] - ETA: 9:26 - loss: 0.2072 - accuracy: 0.9489"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVaffk7zs1S7"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2u-3dL_s1EJ"
      },
      "source": [
        "model = tf.keras.models.load_model('/content/drive/MyDrive/dataset/final_model.h5')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mhehx-J9tUnv",
        "outputId": "a4331da4-a7be-4b2f-ff13-b44f530a5c3f"
      },
      "source": [
        "np.sum( np.argmax(model.predict(X_valid),axis=1) == y_valid) / len(y_valid)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.95486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdSpgY15tCrd"
      },
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/dataset/final_test.csv')\n",
        "#sample = pd.read_csv('/content/drive/MyDrive/dataset/sample_submission.csv')"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbge6RThu34A",
        "outputId": "8a298afe-0a49-4891-f199-7da6e25bd16c"
      },
      "source": [
        "test_data = test['data']\n",
        "test_data.values"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['أَمّا الوِلايَةُ فَالمَعروفُ خُطبَتُها # وَلَستُ أَقبَلُها عَن بابِكُم بَدَلا',\n",
              "       'إِذا مامَرَرتَ بِأَهلِ القُبورِ # تَيَقَّنتَ أَنَّكَ مِنهُم غَدا',\n",
              "       'فعِشْ إنْ قدرتَ قليلَ الحديثِ # قليل الجَليس قليل الخِصام', ...,\n",
              "       'إِنَّ الدِيارَ الَّتي تُبكى بِمُتَّقِدٍ # غَيرُ الدِيارِ الَّتي تَبكي بِهَطّالِ',\n",
              "       'فاخترت هذا ولا العشار من كضض # تركت قلبي كريش النسر منتفضا',\n",
              "       'وَتَرجِعُ لي روحُ الحَياةِ فَإِنَّني # بِنَفسِيَ لَو عايَنتَني لَأَجودُ'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrTTXnepu6pn"
      },
      "source": [
        "def getResults(data):\n",
        "  x_test = pad_sequences(data.apply(lambda x: tokenize(cleanSentence(x))).values,padding='post', value=0, maxlen = 100)\n",
        "  return np.argmax(model.predict(x_test),axis=1) "
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "iwg6YDbfwCsR",
        "outputId": "255f09be-3956-4a0f-9edd-64c2bf9743fe"
      },
      "source": [
        "test['labels'] = getResults(test_data)\n",
        "file_submission = test[['id','labels']]\n",
        "file_submission"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1995</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1996</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1997</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1998</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1999</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  labels\n",
              "0        0       8\n",
              "1        1       2\n",
              "2        2       2\n",
              "3        3      10\n",
              "4        4      11\n",
              "...    ...     ...\n",
              "1995  1995      10\n",
              "1996  1996       9\n",
              "1997  1997       8\n",
              "1998  1998       8\n",
              "1999  1999      10\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2JjKKfIw-ff"
      },
      "source": [
        "file_submission.to_csv('final_model.csv', index=False)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmE15ONWvSLN"
      },
      "source": [
        "# HELPERS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btvqG92nts3M"
      },
      "source": [
        "## Helper Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFTXemHStv2L"
      },
      "source": [
        "def clean_text(string):\n",
        "  \"\"\"\n",
        "  Sources \n",
        "  ----------\n",
        "  https://stackabuse.com/using-regex-for-text-manipulation-in-python/\n",
        "  https://lionbridge.ai/articles/using-natural-language-processing-for-spam-detection-in-emails/\n",
        "  \n",
        "  Description\n",
        "  -----------\n",
        "  clean text by handling unneeded words\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  string : str\n",
        "      string to be processed\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "      cleaned string\n",
        "  \"\"\"\n",
        "  string = str(string) if type(string) != type('aa') else string\n",
        "  string = string.lower()\n",
        "  string = re.sub(r\"http\\S+\", \"\", string)\n",
        "  string = re.sub(r\"\\W\", \" \", string, flags=re.I)\n",
        "  string = re.sub(r\"[^A-Za-z0-9]\", \" \", string)\n",
        "  string = re.sub(r\"\\'s\", \" is \", string)\n",
        "  string = re.sub(r\"\\'ve\", \" have \", string)\n",
        "  string = re.sub(r\"can't\", \"cannot \", string)\n",
        "  string = re.sub(r\"n't\", \" not \", string)\n",
        "  string = re.sub(r\"I'm\", \"I am\", string)\n",
        "  string = re.sub(r\"\\'re\", \" are \", string)\n",
        "  string = re.sub(r\"\\'d\", \" would \", string)\n",
        "  string = re.sub(r\"\\'ll\", \" will \", string)\n",
        "  string = re.sub(r\"e-mail\", \"email\", string)\n",
        "  string = re.sub(r\" usa \", \" america \", string)\n",
        "  string = re.sub(r\" uk \", \" england \", string)\n",
        "  string = re.sub(r\"\\s+\",\" \", string, flags = re.I)\n",
        "  string = string[7:] if re.search(r\"^subject\", string) else string\n",
        "  string = re.sub(r\"^\\s+\", \"\", string)\n",
        "  string = string[7:] if re.search(r\"^re\", string) else string\n",
        "  string = re.sub(r\"^\\s+\", \"\", string)\n",
        "  string = re.sub(r\"\\s+$\", \"\", string)\n",
        "  string = re.sub(r\"\\s+[a-zA-Z]\\s+\", \" \", string)\n",
        "\n",
        "  return string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO1Cs7fMtyCH"
      },
      "source": [
        "def tokenizer(text):\n",
        "  \"\"\"\n",
        "  Sources \n",
        "  ----------\n",
        "  \n",
        "  Description\n",
        "  -----------\n",
        "  Converts text to set of encoded words\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : pd.Series\n",
        "      data to be processed\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  pd.Series\n",
        "      processed data\n",
        "  \"\"\"\n",
        "\n",
        "  tk = Tokenizer(num_words=TOKENIZER_WORD_NUM)\n",
        "  tk.fit_on_texts(text)\n",
        "  return pd.Series(tk.texts_to_sequences(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-VnvfYGt9q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2536ef15-f8ef-43b7-d36d-43616e1682bf"
      },
      "source": [
        "def preprocess_text(text,tokenizer=word_tokenize,stops_remove=True,stemmer = SnowballStemmer('english'),stop_words=stopwords.words('english')):\n",
        "  \"\"\"\n",
        "  Sources \n",
        "  ----------\n",
        "  \n",
        "  Description\n",
        "  -----------\n",
        "  Perform the whole preprocessing pipeline for a given text\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : str\n",
        "      text to be processed\n",
        "\n",
        "  tokenizer: function (optional)\n",
        "      used tokenizer\n",
        "\n",
        "  stemmer: object (optional)\n",
        "      used stemmer\n",
        "\n",
        "  stops_remove: list (optional)\n",
        "      stop words to be removed\n",
        "\n",
        "  \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  str\n",
        "      processed string\n",
        "  \"\"\"\n",
        "  \n",
        "  # (1) Cleaning text\n",
        "  text = clean_text(text)\n",
        "\n",
        "  # (2) Tokenizing\n",
        "  text = tokenizer(text)\n",
        "  \n",
        "  # (3) Removing stopwords\n",
        "  text = [word for word in text if word not in stop_words]\n",
        "\n",
        "  # (4) Stemming\n",
        "  text = [stemmer.stem(word) for word in text]\n",
        "\n",
        "  return text\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-24fe6a6621ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstops_remove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \"\"\"\n\u001b[1;32m      3\u001b[0m   \u001b[0mSources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlOm7IfFuCYg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad0ca3d-ddf7-46ca-9c9e-c523780b9ffc"
      },
      "source": [
        "def preprocess_df(df,X,y,tokenizer=word_tokenize,stops_remove=True,stemmer = SnowballStemmer('english'),stop_words=stopwords.words('english')):\n",
        "  \"\"\"\n",
        "  Sources \n",
        "  ----------\n",
        "  \n",
        "  Description\n",
        "  -----------\n",
        "  Perform the whole preprocessing pipeline for a given dataframe\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  text : pd.DataFrame\n",
        "      dataframe to be processed\n",
        "\n",
        "  tokenizer: function (optional)\n",
        "      used tokenizer\n",
        "\n",
        "  stemmer: object (optional)\n",
        "      used stemmer\n",
        "\n",
        "  stops_remove: list (optional)\n",
        "      stop words to be removed\n",
        "\n",
        "  \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  pd.DataFrame\n",
        "      processed dataframe\n",
        "  \"\"\"\n",
        "  df_unique = df.drop_duplicates()\n",
        "  df_ = pd.DataFrame(df_unique[X].apply(lambda x: preprocess_text(x,tokenizer=tokenizer,stops_remove=stops_remove,stemmer=stemmer,stop_words=stop_words)))  \n",
        "  df_[y] = df_unique[y]\n",
        "  return df_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-917dbbfd18c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstops_remove\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \"\"\"\n\u001b[1;32m      3\u001b[0m   \u001b[0mSources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDDUSO2dvKvN"
      },
      "source": [
        "def prepare_data(df,X,length_col,vocab):\n",
        "  # (1) Encoding words\n",
        "  df[X] = df[X].apply(lambda sentence: [vocab[word] for word in sentence])\n",
        "\n",
        "  # (2) Padding/Truncating rows\n",
        "  length_stats = df[length_col].describe()\n",
        "  common_length = length_stats.loc['75%']\n",
        "  max_length = length_stats.loc['max']\n",
        "  picked_length = int( max_length if common_length / max_length >= 0.5 else common_length)\n",
        "\n",
        "  df[X] = df[X].apply(lambda sentence: sentence + [0]*(picked_length-len(sentence)) if len(sentence) < common_length else sentence[:picked_length])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydi8ofgtuLWh"
      },
      "source": [
        "# Helper Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3EB6RApuNCw"
      },
      "source": [
        "class Vocabulary:\n",
        "  \"\"\"\n",
        "  Description\n",
        "  -----------\n",
        "  - Keeps the words in dataset with count\n",
        "  - Gives a token for each word\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,max_vocab_size=-1):\n",
        "    \"\"\"  \n",
        "    Description\n",
        "    -----------\n",
        "      Initialize vocabulary \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    max_vocab_size : int\n",
        "      maximum vocabulary size\n",
        "    \"\"\"\n",
        "    # Members\n",
        "    self.word_to_index = dict()\n",
        "    self.index_to_word = dict()\n",
        "    self.word_count = pd.Series(dtype=np.int32)\n",
        "    self.unique_word_count = pd.Series(dtype=np.int32)\n",
        "    self.prev_sentence_index = -1\n",
        "    self.vocab_size = 0\n",
        "    \n",
        "    self.max_vocab_size = max_vocab_size\n",
        "\n",
        "    self.word_to_index[' '] = self.vocab_size\n",
        "    self.index_to_word[self.vocab_size] = '<empty>'\n",
        "    self.vocab_size += 1\n",
        "    self.word_count.loc['<empty>'] = GAMMA\n",
        "\n",
        "    self.word_to_index['<unkown>'] = self.vocab_size\n",
        "    self.index_to_word[self.vocab_size] = '<unkown>'\n",
        "    self.vocab_size += 1\n",
        "    self.word_count.loc['<unkown>'] = GAMMA\n",
        "\n",
        "    self.tf_dict = {}\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    -----------\n",
        "      Get the size of vocabulary\n",
        "    \"\"\"\n",
        "    return self.vocab_size\n",
        "\n",
        "  def __getitem__(self,key):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    -----------\n",
        "      Get the size of vocabulary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    key : int/str\n",
        "      Index/word to get its corresponding word/index\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int/str\n",
        "      Query\n",
        "    \"\"\"\n",
        "\n",
        "    # If key is string\n",
        "    if type(key) == type('ss'):\n",
        "      query = 1\n",
        "      try: \n",
        "        query = self.word_to_index[key]\n",
        "      except:\n",
        "        pass\n",
        "      return query\n",
        "    # If key is integer\n",
        "    elif type(key) == type(50):\n",
        "      query = 0\n",
        "      try:\n",
        "        query = self.index_to_word[key]\n",
        "      except:\n",
        "        raise KeyError('Index out of range')\n",
        "      return query\n",
        "    # If key is an unknown type\n",
        "    else:\n",
        "      raise KeyError(\"Invalid key type, key must be string or integer\")\n",
        "\n",
        "\n",
        "  def add_word(self,word,sentence_index=0,sentence_len=1,calculate_tf = False):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    -----------\n",
        "      Add word to the vocabulary\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    word : str\n",
        "      Word to be added\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "      The state of adding the word (success/fail)\n",
        "    \"\"\"   \n",
        "    \n",
        "    try:\n",
        "      self.word_count.loc[word] += 1\n",
        "      if self.prev_sentence_index != sentence_index:\n",
        "        self.unique_word_count[word] += 1\n",
        "        self.prev_sentence_index = sentence_index\n",
        "      if calculate_tf:\n",
        "        try:\n",
        "          self.tf_dict[word][sentence_index] += 1/(sentence_len+GAMMA)\n",
        "        except:\n",
        "          self.tf_dict[word][sentence_index] = 1/(sentence_len+GAMMA)\n",
        "    except:\n",
        "      # If the vocab reached max size\n",
        "      if self.vocab_size == self.max_vocab_size:\n",
        "        return False\n",
        "      # Adding new word\n",
        "      self.word_count.loc[word] = 1\n",
        "      self.unique_word_count.loc[word] = 1\n",
        "      self.prev_sentence_index = sentence_index\n",
        "      self.word_to_index[word] = self.vocab_size\n",
        "      self.index_to_word[self.vocab_size] = word\n",
        "      self.vocab_size += 1\n",
        "      if calculate_tf:\n",
        "        self.tf_dict[word] = {sentence_index:1/(sentence_len+GAMMA)}\n",
        "    \n",
        "    return True\n",
        "\n",
        "  def create_vocab(self,df,X='Body',calculate_tfidf = False):\n",
        "    for index,sentence in enumerate(df[X]):\n",
        "      for word in sentence:\n",
        "        my_vocab.add_word(word,index,len(sentence),calculate_tfidf)\n",
        "    \n",
        "    if calculate_tfidf:\n",
        "      self.create_tfidf_matrix(df)\n",
        "\n",
        "  def get_vocab_words(self):\n",
        "    return self.index_to_word.values()\n",
        "\n",
        "  def create_tfidf_matrix(self,df,X='Body'):\n",
        "    self.tfidf_matrix = np.empty([len(df),self.vocab_size],dtype=np.float64)\n",
        "\n",
        "    for word,dic in self.tf_dict.items():\n",
        "      for index,word_tf in dic.items():\n",
        "        idf = np.log(len(df) / self.unique_word_count.loc[word])\n",
        "        self.tfidf_matrix[index,self.word_to_index[word]] = word_tf * idf\n",
        "\n",
        "  \n",
        "  def add_vectorizer(self,vectorizer):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    -----------\n",
        "      Add vectorized object\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    word : str\n",
        "      Word to be added\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    bool\n",
        "      The state of adding the word (success/fail)\n",
        "    \"\"\"\n",
        "    self.vectorizer = vectorizer\n",
        "\n",
        "  def word_to_vector(self,word):\n",
        "    vector = np.zeros(EMBEDDING_DIM)\n",
        "    try:\n",
        "      vector = self.vectorizer[word]\n",
        "    except:\n",
        "      pass\n",
        "    return vector\n",
        "\n",
        "  def create_embedding_matrix(self):\n",
        "    self.embedding_matrix = np.empty([self.vocab_size,EMBEDDING_DIM],dtype=np.float64)\n",
        "    for index,word in self.index_to_word.items():\n",
        "      self.embedding_matrix[index] = self.word_to_vector(word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfNtQKyrDDCi"
      },
      "source": [
        "# Other Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDqeodX6DGlx"
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "def extract_data(path, thresh = 70, on_shatrs = False):\n",
        "  global vocab\n",
        "  \n",
        "  text = \"\"\n",
        "  \n",
        "  X = []\n",
        "  y = []\n",
        "    \n",
        "  t = open(path, 'r').read()\n",
        "  #t = araby.strip_tashkeel(t)\n",
        "  # remove some exteranous chars \n",
        "  execluded = '!()*-ـ.:=o[]«»;؛,،~?؟\\u200f\\ufeffـ'\n",
        "  out = \"\"\n",
        "  \n",
        "  for char in t:\n",
        "    #if char not in execluded:\n",
        "    out += char\n",
        "      \n",
        "  text += out\n",
        "  baits = out.split('\\n')\n",
        "  for line in baits:\n",
        "    if len(line) <= 1:\n",
        "      continue\n",
        "    label, bait = line.split(' ', 1)\n",
        "    label = int(label)\n",
        "\n",
        "    bait  = bait.strip()\n",
        "    if on_shatrs:\n",
        "      shatrs = bait.split('#')\n",
        "      for shatr in shatrs:\n",
        "        X.append(shatr.strip())\n",
        "        y.append(label)\n",
        "    else:\n",
        "      X.append(bait.strip())\n",
        "      y.append(label)\n",
        "  \n",
        "  #create the vocab \n",
        "  vocab = sorted(set(' '.join(X)))  \n",
        "  \n",
        "  #shuffle the data \n",
        "  X, y = shuffle(X, y)\n",
        "  return X, y"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pk2V3N_DRF0"
      },
      "source": [
        "from sklearn.utils import shuffle\n"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHFWxdMuDMNy"
      },
      "source": [
        "X, y = extract_data(\"final_baits/test.txt\", on_shatrs=False)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "PxABxiaODSmy",
        "outputId": "f09da909-5cc1-430d-bd02-fd53c97f78f2"
      },
      "source": [
        "long_data = pd.DataFrame(X).rename(columns={0:'data'})\n",
        "long_data['labels'] = y\n",
        "long_data"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>data</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>بِكُلِّ تَقديسَةٍ يُرَدِّدُها # فيكَ النَصاري ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>تعالَت ذاتُ مَولايَ # عَن الإِدراكِ بِاللَحظِ</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>بِوادي لَكَ بِالشَوق الَّذي # في فُؤادي لا تدَ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ومن ألفناه كان الموت فرّقنا # لا يمنع الموت جي...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>واسأل الرحمن لي في حاجتي # التي في النفس منها ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8311</th>\n",
              "      <td>دِيَماً في كُلِّ يَومٍ وَوَبلاً # وَاِغتِباقاً...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8312</th>\n",
              "      <td>وَلقدْ أَنْذَرْتُهُ فَرَأَيْتُهُ # جَاهِلِيَّا...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8313</th>\n",
              "      <td>في كفّه نبعةٌ مُوَتَّرة # يهزج ابياضًها ويهتضِبُ</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8314</th>\n",
              "      <td>كَأَنَّهُم لِلعُيون تَبصرة # كَأَنَّهُم في الق...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8315</th>\n",
              "      <td>وَلَئِن طَرِبتُ وَقَد عَرَتني وَعكَةٌ # فَاللَ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8316 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   data  labels\n",
              "0     بِكُلِّ تَقديسَةٍ يُرَدِّدُها # فيكَ النَصاري ...       4\n",
              "1         تعالَت ذاتُ مَولايَ # عَن الإِدراكِ بِاللَحظِ      12\n",
              "2     بِوادي لَكَ بِالشَوق الَّذي # في فُؤادي لا تدَ...       7\n",
              "3     ومن ألفناه كان الموت فرّقنا # لا يمنع الموت جي...       8\n",
              "4     واسأل الرحمن لي في حاجتي # التي في النفس منها ...       7\n",
              "...                                                 ...     ...\n",
              "8311  دِيَماً في كُلِّ يَومٍ وَوَبلاً # وَاِغتِباقاً...       5\n",
              "8312  وَلقدْ أَنْذَرْتُهُ فَرَأَيْتُهُ # جَاهِلِيَّا...       9\n",
              "8313   في كفّه نبعةٌ مُوَتَّرة # يهزج ابياضًها ويهتضِبُ       4\n",
              "8314  كَأَنَّهُم لِلعُيون تَبصرة # كَأَنَّهُم في الق...       4\n",
              "8315  وَلَئِن طَرِبتُ وَقَد عَرَتني وَعكَةٌ # فَاللَ...       1\n",
              "\n",
              "[8316 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxuZz-7SDi82"
      },
      "source": [
        "ver_long_data = pd.concat([data,long_data],ignore_index=True)\n",
        "ver_long_data = ver_long_data.drop(labels=[\"labels_text\"], axis=1)\n",
        "ver_long_data = ver_long_data.drop_duplicates()\n",
        "ver_long_data = ver_long_data.reset_index().drop(labels=[\"index\"], axis=1)"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcdAwagwEZJf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "025d175e-99d8-494d-dac2-1a39db6e7a6b"
      },
      "source": [
        "ver_long_data"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>labels</th>\n",
              "      <th>data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>أَنا الفقير وباللَه العظيم غني # لئن فقدتك في ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>وَلوعاً بِيُمنَى نَمْنَمَتْها حَدِيقَةٌ # نَزْ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>11</td>\n",
              "      <td>فيا منْ لم أزلْ أحظى لديه # بفضلٍ جامعٍ بابَ ا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>وَسَلامٌ عَلَى ضَرِيحِكَ مَا أَهْ # دَتْ شَذَا...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>أمِنْتُ فقري لما قُلتُ عن ثِقَةٍ # أنْ لا جواد...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471171</th>\n",
              "      <td>0</td>\n",
              "      <td>أَثْقَلَنِي بِالْبِرِّ حَتَّى لَقَدْ # أَعْجِز...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471172</th>\n",
              "      <td>8</td>\n",
              "      <td>إلا لحالَيْن فَقْدِ العقلِ والدينِ # لو رام ذل...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471173</th>\n",
              "      <td>7</td>\n",
              "      <td>حارَبوا الجَهلَ وَكانوا قَبلَنا # في دُجى عَمي...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471174</th>\n",
              "      <td>9</td>\n",
              "      <td>فَقَطَفْتُ الشَّقِيقَ منْ وجْهِهِ # وَاغْتَبَق...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471175</th>\n",
              "      <td>13</td>\n",
              "      <td>فَذاكَ جذرُ المَالِ بالنُّقصان # وَذاكَ جَذرُ ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1471176 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         labels                                               data\n",
              "0             8  أَنا الفقير وباللَه العظيم غني # لئن فقدتك في ...\n",
              "1            10  وَلوعاً بِيُمنَى نَمْنَمَتْها حَدِيقَةٌ # نَزْ...\n",
              "2            11  فيا منْ لم أزلْ أحظى لديه # بفضلٍ جامعٍ بابَ ا...\n",
              "3             9  وَسَلامٌ عَلَى ضَرِيحِكَ مَا أَهْ # دَتْ شَذَا...\n",
              "4             8  أمِنْتُ فقري لما قُلتُ عن ثِقَةٍ # أنْ لا جواد...\n",
              "...         ...                                                ...\n",
              "1471171       0  أَثْقَلَنِي بِالْبِرِّ حَتَّى لَقَدْ # أَعْجِز...\n",
              "1471172       8  إلا لحالَيْن فَقْدِ العقلِ والدينِ # لو رام ذل...\n",
              "1471173       7  حارَبوا الجَهلَ وَكانوا قَبلَنا # في دُجى عَمي...\n",
              "1471174       9  فَقَطَفْتُ الشَّقِيقَ منْ وجْهِهِ # وَاغْتَبَق...\n",
              "1471175      13  فَذاكَ جذرُ المَالِ بالنُّقصان # وَذاكَ جَذرُ ...\n",
              "\n",
              "[1471176 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJyi54scRAOQ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}